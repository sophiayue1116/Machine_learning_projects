{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,GlobalAveragePooling2D,Input,Activation\n",
    "from keras.layers.merge import add\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.activations import relu,softmax\n",
    "from keras import optimizers,applications,regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import TensorBoard,ReduceLROnPlateau,ModelCheckpoint\n",
    "from tensorflow.keras import backend\n",
    "from keras.models import model_from_json\n",
    "\n",
    "seed = 42\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (42000, 784)\n",
      "Shape of y_train: (42000,)\n",
      "Shape of X_test : (28000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train.drop(['label'], axis=1)\n",
    "y_train = df_train['label']\n",
    "\n",
    "#df_train['label'] = df_train['label'].replace(to_replace=np.nan, value=0.0)\n",
    "#df_train['label'] = df_train['label'].replace(to_replace='', value=0.0)\n",
    "#y_train_ = df_train['label'].values.astype(np.int).reshape(-1, 1)\n",
    "#X_train = df_train[df_train.columns[1:]].values.astype(np.float32).reshape((-1, 28, 28, 1))\n",
    "\n",
    "del df_train\n",
    "X_test = df_test\n",
    "\n",
    "del df_test\n",
    "\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "print('Shape of X_test :', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (42000, 28, 28, 1)\n",
      "Shape of X_test : (28000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.values.reshape(-1,28,28,1) \n",
    "X_test = X_test.values.reshape(-1,28,28,1)\n",
    "\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of X_test :', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding\n",
    "\n",
    "#y_train = OneHotEncoder(sparse=False).fit_transform(y_train_)\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = 1\n",
    "digits = [0] * nets\n",
    "history = [0] * nets\n",
    "epochs = 100\n",
    "batch_size = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                 patience=3, \n",
    "                                 verbose=0, \n",
    "                                 factor=0.5, \n",
    "                                 min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "imggen = ImageDataGenerator(featurewise_center = False,\n",
    "                               samplewise_center = False, \n",
    "                               featurewise_std_normalization = False,\n",
    "                               samplewise_std_normalization = False,\n",
    "                               zca_whitening = False,\n",
    "                               rotation_range = 10, \n",
    "                               zoom_range = 0.10, \n",
    "                               width_shift_range = 0.10, \n",
    "                               height_shift_range = 0.10, \n",
    "                               horizontal_flip = False,\n",
    "                               vertical_flip = False)\n",
    "\n",
    "imggen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline():\n",
    "    pipeline = Sequential()    \n",
    "    \n",
    "    # Gridsearch\n",
    "\n",
    "    pipeline.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation ='relu', \n",
    "                 input_shape = (28, 28, 1)))\n",
    "    pipeline.add(BatchNormalization())\n",
    "    pipeline.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation ='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    pipeline.add(BatchNormalization())\n",
    "    pipeline.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation ='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    pipeline.add(BatchNormalization())\n",
    "\n",
    "    #Maxpooling to reduce time complexity\n",
    "    #pipeline.add(MaxPool2D(pool_size=(2,2)))\n",
    "    pipeline.add(MaxPool2D(pool_size=(4,4)))\n",
    "\n",
    "    # Regularization    \n",
    "    #pipeline.add(Dropout(0.25))\n",
    "    pipeline.add(Dropout(0.15))\n",
    "\n",
    "    # Last layers\n",
    "\n",
    "    pipeline.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    pipeline.add(BatchNormalization())\n",
    "    pipeline.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    pipeline.add(BatchNormalization())\n",
    "    pipeline.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    pipeline.add(BatchNormalization())  \n",
    "    pipeline.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    pipeline.add(Dropout(0.25))\n",
    "\n",
    "    #Flatten\n",
    "    pipeline.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same', activation ='sigmoid', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    pipeline.add(BatchNormalization())\n",
    "\n",
    "    pipeline.add(Flatten())\n",
    "    pipeline.add(Dense(128, activation = \"relu\"))\n",
    "    pipeline.add(Dropout(0.4))\n",
    "\n",
    "    # Depth of connected layers with probablistic categorization \n",
    "    pipeline.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "    pipeline.compile(optimizer = 'Adamax' , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1 CNNs...\n",
      "CNN  1: Epochs = 100, Max. Train accuracy = 0.99815, Max. Validation accuracy = 0.99643\n"
     ]
    }
   ],
   "source": [
    "print('Creating {0} CNNs...'.format(nets))\n",
    "for model in range(nets):\n",
    "    \n",
    "    digits[model] = build_pipeline()\n",
    "    \n",
    "    # Splitting train and test datasets\n",
    "    \n",
    "    X_train_aux, X_test_aux, y_train_aux, y_test_aux = train_test_split(X_train, y_train, test_size = 0.1)\n",
    "    \n",
    "    history[model] = digits[model].fit_generator(imggen.flow(X_train_aux,\n",
    "                                                              y_train_aux, \n",
    "                                                              batch_size = batch_size),\n",
    "                                                 epochs = epochs, \n",
    "                                                 steps_per_epoch = X_train_aux.shape[0] // batch_size, \n",
    "                                                 validation_data = (X_test_aux, y_test_aux), \n",
    "                                                 callbacks=[reduction],\n",
    "                                                 verbose=0)\n",
    "    \n",
    "    print(\"CNN {0:>2d}: Epochs = {1:d}, Max. Train accuracy = {2:.5f}, Max. Validation accuracy = {3:.5f}\".format(\n",
    "        model + 1,\n",
    "        epochs, \n",
    "        max(history[model].history['acc']), \n",
    "        max(history[model].history['val_acc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label = np.zeros( (X_test.shape[0], 10) )\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in range(nets):\n",
    "    predicted_label = predicted_label + digits[model].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max prob index\n",
    "predicted_label = np.argmax(predicted_label, axis = 1)\n",
    "predicted_label = pd.Series(predicted_label, name = \"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([pd.Series(range(1, 28001), name = \"ImageId\"), predicted_label], axis = 1)\n",
    "result.to_csv(\"result_optimized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId  Label\n",
       "0        1      2\n",
       "1        2      0\n",
       "2        3      9\n",
       "3        4      0\n",
       "4        5      3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37800"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(np.argmax(y_train_aux[0:28000],axis=1), predicted_label, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn,fp,fn,tp = matrix.ravel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
